{"cells":[{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["Found cached dataset squad (/storagenfs/m.tolloso/.cache/huggingface/datasets/squad/plain_text/1.0.0/d6ec3ceb99ca480ce37cdd35555d6cb2511d223b9150cce08a837ef62ffea453)\n","100%|██████████| 2/2 [00:00<00:00, 131.25it/s]\n","Loading cached processed dataset at /storagenfs/m.tolloso/.cache/huggingface/datasets/squad/plain_text/1.0.0/d6ec3ceb99ca480ce37cdd35555d6cb2511d223b9150cce08a837ef62ffea453/cache-b6fc25269567b932.arrow\n","Loading cached processed dataset at /storagenfs/m.tolloso/.cache/huggingface/datasets/squad/plain_text/1.0.0/d6ec3ceb99ca480ce37cdd35555d6cb2511d223b9150cce08a837ef62ffea453/cache-8bdf5825ed0d5102.arrow\n","Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"]},{"ename":"TypeError","evalue":"PreTrainedTokenizerFast._batch_encode_plus() got an unexpected keyword argument 'device'","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[1;32m/storagenfs/m.tolloso/HLT_Project/CheckAndswers.ipynb Cell 1\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bc4130-p100.unipi.it/storagenfs/m.tolloso/HLT_Project/CheckAndswers.ipynb#W0sdnNjb2RlLXJlbW90ZQ%3D%3D?line=19'>20</a>\u001b[0m counter \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bc4130-p100.unipi.it/storagenfs/m.tolloso/HLT_Project/CheckAndswers.ipynb#W0sdnNjb2RlLXJlbW90ZQ%3D%3D?line=20'>21</a>\u001b[0m \u001b[39mfor\u001b[39;00m context \u001b[39min\u001b[39;00m contexts:\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bc4130-p100.unipi.it/storagenfs/m.tolloso/HLT_Project/CheckAndswers.ipynb#W0sdnNjb2RlLXJlbW90ZQ%3D%3D?line=21'>22</a>\u001b[0m     tokenized_in_ag \u001b[39m=\u001b[39m tokenizer_ag(\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bc4130-p100.unipi.it/storagenfs/m.tolloso/HLT_Project/CheckAndswers.ipynb#W0sdnNjb2RlLXJlbW90ZQ%3D%3D?line=22'>23</a>\u001b[0m             context,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bc4130-p100.unipi.it/storagenfs/m.tolloso/HLT_Project/CheckAndswers.ipynb#W0sdnNjb2RlLXJlbW90ZQ%3D%3D?line=23'>24</a>\u001b[0m             max_length\u001b[39m=\u001b[39;49m\u001b[39m256\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bc4130-p100.unipi.it/storagenfs/m.tolloso/HLT_Project/CheckAndswers.ipynb#W0sdnNjb2RlLXJlbW90ZQ%3D%3D?line=24'>25</a>\u001b[0m             truncation\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39monly_first\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bc4130-p100.unipi.it/storagenfs/m.tolloso/HLT_Project/CheckAndswers.ipynb#W0sdnNjb2RlLXJlbW90ZQ%3D%3D?line=25'>26</a>\u001b[0m             padding\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mmax_length\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bc4130-p100.unipi.it/storagenfs/m.tolloso/HLT_Project/CheckAndswers.ipynb#W0sdnNjb2RlLXJlbW90ZQ%3D%3D?line=26'>27</a>\u001b[0m             return_tensors\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mpt\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bc4130-p100.unipi.it/storagenfs/m.tolloso/HLT_Project/CheckAndswers.ipynb#W0sdnNjb2RlLXJlbW90ZQ%3D%3D?line=27'>28</a>\u001b[0m             device\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mcuda\u001b[39;49m\u001b[39m'\u001b[39;49m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bc4130-p100.unipi.it/storagenfs/m.tolloso/HLT_Project/CheckAndswers.ipynb#W0sdnNjb2RlLXJlbW90ZQ%3D%3D?line=28'>29</a>\u001b[0m         )\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bc4130-p100.unipi.it/storagenfs/m.tolloso/HLT_Project/CheckAndswers.ipynb#W0sdnNjb2RlLXJlbW90ZQ%3D%3D?line=30'>31</a>\u001b[0m     input_ids_ag \u001b[39m=\u001b[39m tokenized_in_ag[\u001b[39m\"\u001b[39m\u001b[39minput_ids\u001b[39m\u001b[39m\"\u001b[39m][\u001b[39m0\u001b[39m]\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bc4130-p100.unipi.it/storagenfs/m.tolloso/HLT_Project/CheckAndswers.ipynb#W0sdnNjb2RlLXJlbW90ZQ%3D%3D?line=33'>34</a>\u001b[0m     output \u001b[39m=\u001b[39m model_ag\u001b[39m.\u001b[39mgenerate(\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bc4130-p100.unipi.it/storagenfs/m.tolloso/HLT_Project/CheckAndswers.ipynb#W0sdnNjb2RlLXJlbW90ZQ%3D%3D?line=34'>35</a>\u001b[0m         input_ids_ag\u001b[39m.\u001b[39mreshape(\u001b[39m1\u001b[39m, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m), \n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bc4130-p100.unipi.it/storagenfs/m.tolloso/HLT_Project/CheckAndswers.ipynb#W0sdnNjb2RlLXJlbW90ZQ%3D%3D?line=35'>36</a>\u001b[0m         num_beams\u001b[39m=\u001b[39m\u001b[39m20\u001b[39m, \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bc4130-p100.unipi.it/storagenfs/m.tolloso/HLT_Project/CheckAndswers.ipynb#W0sdnNjb2RlLXJlbW90ZQ%3D%3D?line=38'>39</a>\u001b[0m         num_return_sequences\u001b[39m=\u001b[39m\u001b[39m10\u001b[39m,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bc4130-p100.unipi.it/storagenfs/m.tolloso/HLT_Project/CheckAndswers.ipynb#W0sdnNjb2RlLXJlbW90ZQ%3D%3D?line=39'>40</a>\u001b[0m     )\n","File \u001b[0;32m~/miniconda3/envs/hlt/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2806\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.__call__\u001b[0;34m(self, text, text_pair, text_target, text_pair_target, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   2804\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_in_target_context_manager:\n\u001b[1;32m   2805\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_switch_to_input_mode()\n\u001b[0;32m-> 2806\u001b[0m     encodings \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_one(text\u001b[39m=\u001b[39;49mtext, text_pair\u001b[39m=\u001b[39;49mtext_pair, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mall_kwargs)\n\u001b[1;32m   2807\u001b[0m \u001b[39mif\u001b[39;00m text_target \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   2808\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_switch_to_target_mode()\n","File \u001b[0;32m~/miniconda3/envs/hlt/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2912\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase._call_one\u001b[0;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   2892\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch_encode_plus(\n\u001b[1;32m   2893\u001b[0m         batch_text_or_text_pairs\u001b[39m=\u001b[39mbatch_text_or_text_pairs,\n\u001b[1;32m   2894\u001b[0m         add_special_tokens\u001b[39m=\u001b[39madd_special_tokens,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2909\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[1;32m   2910\u001b[0m     )\n\u001b[1;32m   2911\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 2912\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mencode_plus(\n\u001b[1;32m   2913\u001b[0m         text\u001b[39m=\u001b[39;49mtext,\n\u001b[1;32m   2914\u001b[0m         text_pair\u001b[39m=\u001b[39;49mtext_pair,\n\u001b[1;32m   2915\u001b[0m         add_special_tokens\u001b[39m=\u001b[39;49madd_special_tokens,\n\u001b[1;32m   2916\u001b[0m         padding\u001b[39m=\u001b[39;49mpadding,\n\u001b[1;32m   2917\u001b[0m         truncation\u001b[39m=\u001b[39;49mtruncation,\n\u001b[1;32m   2918\u001b[0m         max_length\u001b[39m=\u001b[39;49mmax_length,\n\u001b[1;32m   2919\u001b[0m         stride\u001b[39m=\u001b[39;49mstride,\n\u001b[1;32m   2920\u001b[0m         is_split_into_words\u001b[39m=\u001b[39;49mis_split_into_words,\n\u001b[1;32m   2921\u001b[0m         pad_to_multiple_of\u001b[39m=\u001b[39;49mpad_to_multiple_of,\n\u001b[1;32m   2922\u001b[0m         return_tensors\u001b[39m=\u001b[39;49mreturn_tensors,\n\u001b[1;32m   2923\u001b[0m         return_token_type_ids\u001b[39m=\u001b[39;49mreturn_token_type_ids,\n\u001b[1;32m   2924\u001b[0m         return_attention_mask\u001b[39m=\u001b[39;49mreturn_attention_mask,\n\u001b[1;32m   2925\u001b[0m         return_overflowing_tokens\u001b[39m=\u001b[39;49mreturn_overflowing_tokens,\n\u001b[1;32m   2926\u001b[0m         return_special_tokens_mask\u001b[39m=\u001b[39;49mreturn_special_tokens_mask,\n\u001b[1;32m   2927\u001b[0m         return_offsets_mapping\u001b[39m=\u001b[39;49mreturn_offsets_mapping,\n\u001b[1;32m   2928\u001b[0m         return_length\u001b[39m=\u001b[39;49mreturn_length,\n\u001b[1;32m   2929\u001b[0m         verbose\u001b[39m=\u001b[39;49mverbose,\n\u001b[1;32m   2930\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m   2931\u001b[0m     )\n","File \u001b[0;32m~/miniconda3/envs/hlt/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2985\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.encode_plus\u001b[0;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   2975\u001b[0m \u001b[39m# Backward compatibility for 'truncation_strategy', 'pad_to_max_length'\u001b[39;00m\n\u001b[1;32m   2976\u001b[0m padding_strategy, truncation_strategy, max_length, kwargs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_padding_truncation_strategies(\n\u001b[1;32m   2977\u001b[0m     padding\u001b[39m=\u001b[39mpadding,\n\u001b[1;32m   2978\u001b[0m     truncation\u001b[39m=\u001b[39mtruncation,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2982\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[1;32m   2983\u001b[0m )\n\u001b[0;32m-> 2985\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_encode_plus(\n\u001b[1;32m   2986\u001b[0m     text\u001b[39m=\u001b[39;49mtext,\n\u001b[1;32m   2987\u001b[0m     text_pair\u001b[39m=\u001b[39;49mtext_pair,\n\u001b[1;32m   2988\u001b[0m     add_special_tokens\u001b[39m=\u001b[39;49madd_special_tokens,\n\u001b[1;32m   2989\u001b[0m     padding_strategy\u001b[39m=\u001b[39;49mpadding_strategy,\n\u001b[1;32m   2990\u001b[0m     truncation_strategy\u001b[39m=\u001b[39;49mtruncation_strategy,\n\u001b[1;32m   2991\u001b[0m     max_length\u001b[39m=\u001b[39;49mmax_length,\n\u001b[1;32m   2992\u001b[0m     stride\u001b[39m=\u001b[39;49mstride,\n\u001b[1;32m   2993\u001b[0m     is_split_into_words\u001b[39m=\u001b[39;49mis_split_into_words,\n\u001b[1;32m   2994\u001b[0m     pad_to_multiple_of\u001b[39m=\u001b[39;49mpad_to_multiple_of,\n\u001b[1;32m   2995\u001b[0m     return_tensors\u001b[39m=\u001b[39;49mreturn_tensors,\n\u001b[1;32m   2996\u001b[0m     return_token_type_ids\u001b[39m=\u001b[39;49mreturn_token_type_ids,\n\u001b[1;32m   2997\u001b[0m     return_attention_mask\u001b[39m=\u001b[39;49mreturn_attention_mask,\n\u001b[1;32m   2998\u001b[0m     return_overflowing_tokens\u001b[39m=\u001b[39;49mreturn_overflowing_tokens,\n\u001b[1;32m   2999\u001b[0m     return_special_tokens_mask\u001b[39m=\u001b[39;49mreturn_special_tokens_mask,\n\u001b[1;32m   3000\u001b[0m     return_offsets_mapping\u001b[39m=\u001b[39;49mreturn_offsets_mapping,\n\u001b[1;32m   3001\u001b[0m     return_length\u001b[39m=\u001b[39;49mreturn_length,\n\u001b[1;32m   3002\u001b[0m     verbose\u001b[39m=\u001b[39;49mverbose,\n\u001b[1;32m   3003\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m   3004\u001b[0m )\n","File \u001b[0;32m~/miniconda3/envs/hlt/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py:544\u001b[0m, in \u001b[0;36mPreTrainedTokenizerFast._encode_plus\u001b[0;34m(self, text, text_pair, add_special_tokens, padding_strategy, truncation_strategy, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m    522\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_encode_plus\u001b[39m(\n\u001b[1;32m    523\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    524\u001b[0m     text: Union[TextInput, PreTokenizedInput],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    541\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[1;32m    542\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m BatchEncoding:\n\u001b[1;32m    543\u001b[0m     batched_input \u001b[39m=\u001b[39m [(text, text_pair)] \u001b[39mif\u001b[39;00m text_pair \u001b[39melse\u001b[39;00m [text]\n\u001b[0;32m--> 544\u001b[0m     batched_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_batch_encode_plus(\n\u001b[1;32m    545\u001b[0m         batched_input,\n\u001b[1;32m    546\u001b[0m         is_split_into_words\u001b[39m=\u001b[39;49mis_split_into_words,\n\u001b[1;32m    547\u001b[0m         add_special_tokens\u001b[39m=\u001b[39;49madd_special_tokens,\n\u001b[1;32m    548\u001b[0m         padding_strategy\u001b[39m=\u001b[39;49mpadding_strategy,\n\u001b[1;32m    549\u001b[0m         truncation_strategy\u001b[39m=\u001b[39;49mtruncation_strategy,\n\u001b[1;32m    550\u001b[0m         max_length\u001b[39m=\u001b[39;49mmax_length,\n\u001b[1;32m    551\u001b[0m         stride\u001b[39m=\u001b[39;49mstride,\n\u001b[1;32m    552\u001b[0m         pad_to_multiple_of\u001b[39m=\u001b[39;49mpad_to_multiple_of,\n\u001b[1;32m    553\u001b[0m         return_tensors\u001b[39m=\u001b[39;49mreturn_tensors,\n\u001b[1;32m    554\u001b[0m         return_token_type_ids\u001b[39m=\u001b[39;49mreturn_token_type_ids,\n\u001b[1;32m    555\u001b[0m         return_attention_mask\u001b[39m=\u001b[39;49mreturn_attention_mask,\n\u001b[1;32m    556\u001b[0m         return_overflowing_tokens\u001b[39m=\u001b[39;49mreturn_overflowing_tokens,\n\u001b[1;32m    557\u001b[0m         return_special_tokens_mask\u001b[39m=\u001b[39;49mreturn_special_tokens_mask,\n\u001b[1;32m    558\u001b[0m         return_offsets_mapping\u001b[39m=\u001b[39;49mreturn_offsets_mapping,\n\u001b[1;32m    559\u001b[0m         return_length\u001b[39m=\u001b[39;49mreturn_length,\n\u001b[1;32m    560\u001b[0m         verbose\u001b[39m=\u001b[39;49mverbose,\n\u001b[1;32m    561\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m    562\u001b[0m     )\n\u001b[1;32m    564\u001b[0m     \u001b[39m# Return tensor is None, then we can remove the leading batch axis\u001b[39;00m\n\u001b[1;32m    565\u001b[0m     \u001b[39m# Overflowing tokens are returned as a batch of output so we keep them in this case\u001b[39;00m\n\u001b[1;32m    566\u001b[0m     \u001b[39mif\u001b[39;00m return_tensors \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m return_overflowing_tokens:\n","\u001b[0;31mTypeError\u001b[0m: PreTrainedTokenizerFast._batch_encode_plus() got an unexpected keyword argument 'device'"]}],"source":["import torch\n","from datasets import load_dataset\n","from transformers import AutoTokenizer\n","from transformers import AutoModelForSeq2SeqLM\n","\n","\n","squad = load_dataset(\"squad\")\n","\n","squad = squad.filter(lambda x: len(x['answers']['text'][0].split(' ')) <= 10)\n","\n","\n","contexts = set()\n","for item in squad['validation']:\n","    contexts.add(item['context'])\n","\n","tokenizer_ag = AutoTokenizer.from_pretrained(\"/storagenfs/m.tolloso/HLT_Project/models/t5-small_asnwergen_short/checkpoint-5000\")\n","model_ag = AutoModelForSeq2SeqLM.from_pretrained(\"/storagenfs/m.tolloso/HLT_Project/models/t5-small_asnwergen_short/checkpoint-5000\")\n","\n","contexts = list(contexts)[:100]\n","counter = 0\n","for context in contexts:\n","    tokenized_in_ag = tokenizer_ag(\n","            context,\n","            max_length=256,\n","            truncation=\"only_first\",\n","            padding=\"max_length\",\n","            return_tensors=\"pt\",\n","        )\n","\n","    input_ids_ag = tokenized_in_ag[\"input_ids\"][0]\n","\n","\n","    output = model_ag.generate(\n","        input_ids_ag.reshape(1, -1), \n","        num_beams=20, \n","        # max_length=10,\n","        decoder_start_token_id=tokenizer_ag.pad_token_id,\n","        num_return_sequences=10,\n","    )\n","    answers = tokenizer_ag.batch_decode(output, skip_special_tokens=False)\n","    answers = [ans.strip('<pad>')[1:-3] for ans in answers]\n","    for answer in answers:\n","        # if more than 10 words increment counter\n","        if len(answer.split(\" \")) > 10:\n","            counter += 1\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"data":{"text/plain":["143"]},"execution_count":18,"metadata":{},"output_type":"execute_result"}],"source":["counter"]}],"metadata":{"kernelspec":{"display_name":"hlt","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"}},"nbformat":4,"nbformat_minor":2}
